{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCd3EWZrC7+gCvZjJpK9Pk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rbrtstrk28/machine-learning/blob/main/Assignment_1_Digit_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X24-VJ2kBpGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Recognize a Digit using Machine Learning\n"
      ],
      "metadata": {
        "id": "-YdAZ4ZhDS1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import Libraries\n",
        "We import TensorFlow and other required modules.\n"
      ],
      "metadata": {
        "id": "F7pwogtNDZYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "oKWZ_BjIDkdX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load and Preprocess Data\n",
        "We normalize the images and convert the labels into one-hot encoding.\n"
      ],
      "metadata": {
        "id": "1j7-SZVBDpJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "print(\"Train shape:\", train_images.shape)\n",
        "print(\"Test shape:\", test_images.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mBNuJsuDtPA",
        "outputId": "873a5d61-7b25-475c-fd7a-bfce356c5bed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (60000, 28, 28, 1)\n",
            "Test shape: (10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Build the Model\n",
        "We use a Convolutional Neural Network (CNN) for image classification.\n"
      ],
      "metadata": {
        "id": "r1gFBaxnEjlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHdOUoBOEppz",
        "outputId": "9aa963e3-7b71-49cb-8c82-f0bbeb8ae426"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Compile the Model\n",
        "We use Adam optimizer and categorical cross-entropy loss.\n"
      ],
      "metadata": {
        "id": "lAbmO17VEviQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "im8k_8JXE34s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 5: Train the Model\n",
        "We train the model on training data for 5 epochs.\n"
      ],
      "metadata": {
        "id": "1SOZRg93E-32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVkGinbnFFR8",
        "outputId": "22456a7a-bdd2-4b01-bfab-4479e5b2e503"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - accuracy: 0.8754 - loss: 0.4351 - val_accuracy: 0.9817 - val_loss: 0.0763\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 30ms/step - accuracy: 0.9763 - loss: 0.0817 - val_accuracy: 0.9842 - val_loss: 0.0601\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - accuracy: 0.9839 - loss: 0.0536 - val_accuracy: 0.9870 - val_loss: 0.0483\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.9886 - loss: 0.0361 - val_accuracy: 0.9880 - val_loss: 0.0457\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.9916 - loss: 0.0273 - val_accuracy: 0.9887 - val_loss: 0.0468\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f7e78683d50>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Evaluate the Model\n",
        "We test the accuracy on unseen test data.\n"
      ],
      "metadata": {
        "id": "prKnQiJrFI5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCmajvbBFMn1",
        "outputId": "c872ef9c-6f5b-4884-8b1d-c9a105d52261"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9786 - loss: 0.0579\n",
            "Test Accuracy: 0.9840999841690063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Upload Handwritten Digit Image\n",
        "We'll now test if the model can recognize our own handwritten digits. First, we upload an image (e.g., `my_digit.png`) written on paper or drawn digitally.\n"
      ],
      "metadata": {
        "id": "CaMd2wq0KAcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "KcuTfzJgKGmD",
        "outputId": "713f2024-64df-477c-ada4-488bf0fae7a5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bbead2e6-f6ee-4f68-8e82-b7e6d6413764\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bbead2e6-f6ee-4f68-8e82-b7e6d6413764\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 3.jpg to 3.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Preprocess the Uploaded Image\n",
        "We need to convert the uploaded image into the same format as the MNIST digits:\n",
        "- Resize to 28x28 pixels\n",
        "- Invert colors (if needed)\n",
        "- Normalize pixel values\n",
        "- Reshape to match model input\n"
      ],
      "metadata": {
        "id": "UiASUN5ZKMtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the uploaded file name\n",
        "file_name = next(iter(uploaded))\n",
        "\n",
        "# Load the image in grayscale\n",
        "img = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# ✅ Resize to 28x28 (this line is important – DO NOT make it 140x140!)\n",
        "img = cv2.resize(img, (28, 28))\n",
        "\n",
        "# Invert colors if needed\n",
        "img = 255 - img\n",
        "\n",
        "# Normalize\n",
        "img = img.astype('float32') / 255.0\n",
        "\n",
        "# Reshape to match model input\n",
        "img = img.reshape(1, 28, 28, 1)\n",
        "\n",
        "# Show image\n",
        "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
        "plt.title(\"Processed Handwritten Digit\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "5V0wqRGaKQM4",
        "outputId": "83b4aa54-ff03-44ea-e391-4e4b61f89379"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Predict the Handwritten Digit\n",
        "Now we feed the preprocessed image into the trained model and let it predict what digit it sees.\n"
      ],
      "metadata": {
        "id": "Z4seMTxeKVmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the trained CNN model\n",
        "prediction = model.predict(img)\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "print(\" The model predicts your digit is:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9gWzF_0KYJK",
        "outputId": "ce760533-434d-4619-a70a-a9524b7c6c69"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            " The model predicts your digit is: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "We successfully trained a Convolutional Neural Network to recognize handwritten digits and even tested it on a digit we wrote ourselves. The model performs well and generalizes to new inputs.\n"
      ],
      "metadata": {
        "id": "EpC7YjHKKdPa"
      }
    }
  ]
}